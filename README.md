# ü§ñ RAG Pipeline + RQ Workers
[![RAG Pipeline Demo](https://res.cloudinary.com/dlvcibxgx/video/upload/so_0/preview_ka1nxk.jpg)](https://res.cloudinary.com/dlvcibxgx/video/upload/v1768463666/preview_ka1nxk.mp4)



## Overview
This project implements a Retrieval-Augmented Generation (RAG) pipeline with:
- Local embeddings using **Ollama (nomic-embed-text)**
- Vector storage via **Qdrant**
- Retrieval using **Gemini API / OpenAI SDK**
- API server using **FastAPI + Uvicorn**
- Background + parallel job execution using **RQ (Redis Queue)**

Goal: Retrieve context from a PDF, embed + index it, then answer user queries using grounded retrieval.

---

# ‚öôÔ∏è Worker Setup (RQ on Windows, Linux, macOS)

## RQ on Different OS
### Linux / macOS
- Python supports `os.fork()`, so the default RQ worker works.
- To run a worker:
```bash
rq worker YOUR_QUEUE_NAME
```
- If using macOS and you get fork safety errors:
```bash
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
rq worker YOUR_QUEUE_NAME
```

### Windows
Windows does **not support `fork()`**, so:
- You **must** use `SimpleWorker`(recommended) or `SpawnWorker`. (this can further fail due to os.wait4() command not being available)

#### 1. SpawnWorker (for running `worker.py`)
```bash
rq worker -w rq.worker.SpawnWorker
```

#### 2. SimpleWorker (recommended for Windows dev)
```bash
rq worker -w rq.worker.SimpleWorker
```

#### Worker Pool (Windows) 
#### helps in running queries in true parallelism.
```bash
rq worker-pool -w rq.worker.SimpleWorker -n 3
```

---

# üîß RQ Important Notes

## Redis URL config
RQ assumes:
```
redis://localhost:6379
```
If you're using a different port/domain:
```bash
rq worker -w rq.worker.SimpleWorker --url redis://YOUR_HOST:YOUR_PORT
```

## Worker Status
Check how many workers are running:
```bash
rq info
```

If Redis (Valkey) was stopped and workers still show up:
- They will be auto-removed after **420 seconds** (zombie cleanup).

---

# üì¶ Job Registries (Success & Failure)

## Success Registry
- Completed jobs go here.
- Default TTL = **500 seconds**
- After TTL expiry ‚Üí Redis deletes them to save memory.

## Failure Registry
- Failed jobs go here.
- They **do not** have TTL by default.
- You can:
  - Retry them
  - Delete manually
  - Set custom TTL

---

# üß† Project Context (Architecture)

I built a custom RAG pipeline using:
- **RQ** ‚Üí concurrency + parallelism
- **Ollama** (nomic-embed-text) ‚Üí local embeddings
- **Gemini API** & **OpenAI SDK** ‚Üí retrieval + generation
- **LangChain** ‚Üí indexing, chunking, document loading
- **Qdrant** ‚Üí vector storage
- **FastAPI + Uvicorn** ‚Üí async API service

---

# üóÇ Project Structure (Suggested)
- `components/`
  - `index.py` ‚Äî load ‚Üí chunk ‚Üí embed ‚Üí store in Qdrant
  - `redis_client.py` ‚Äî initiallize redis client for enqueuing jobs.
  - `worker.py` ‚Äî RQ worker code
  - `nodejs.pdf` ‚Äî example document
- `main.py` ‚Äî API entry point
- `.env` ‚Äî secrets, API keys
- `docker-compose.yml` ‚Äî Qdrant setup
- `pyproject.toml` / `uv.lock` ‚Äî dependencies

---

# üöÄ Setup & Installation

## 1. Install uv
```bash
pip install uv
```

## 2. Create Virtual Environment
```bash
uv venv
```

Activate it:

**Linux / macOS**
```bash
source .venv/bin/activate
```

**Windows**
```bash
.venv\Scripts\activate
```

Install dependencies:
```bash
uv sync
```

---

## 3. Add API Keys
Inside `.env`:
```env
GEMINI_API_KEY=""
OPENAI_API_KEY="" (if you are using openai llm instead of gemini)
QDRANT_URL="http://localhost:6333" (not needed if running locally.)
```

---

# üìÑ Document Indexing (Ollama + Qdrant)

## Requirements/ Prerequisite
- Docker installed
- nomic-embed-text model pulled

## Steps

### 1. Start Qdrant, ollama and valkey
```bash
docker compose up -d
```

### 2. Run indexing
```bash
uv run components/index.py
```

---

# üîç Retrieval (Gemini / OpenAI + Qdrant) By üßµ Running the API Server

You can adjust:
- chunk size
- chunk overlap
- top-k retrieved documents

inside the script.
 
```bash
uvicorn main:app --reload
```
- go to http://localhost:8080/docs and try out the /chat and /job-status endpoints.
---

# üß© Summary
You now have:
- Local embeddings
- Local vector database
- RQ-powered async workers
- Gemini/OpenAI retrieval
- Fully functional RAG pipeline
- Cross-platform worker support
